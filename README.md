# Trust-and-Teach AI DApp

Reinforcement learning with human feedback (RLHF) on-chain, leverging on-chain trust, scalability, transparensy, and reputation.

<!--START_SECTION:update_image-->
<img src="./diagrams/usersequence.mmd.svg?jl">
<!--END_SECTION:update_image-->

## Motivation

Higher accuracy for specific application as well as a practical interface, such as chat, of large language models (LLMs) comes from supervised fine-tunning (SFT) and RLHF [A Survey of Large Language Models (2023)](https://arxiv.org/abs/2303.18223).
In RLHF, a human ranks responses from the same prompt of an already fine-tuned model. 
These ranks are used to fine-tune the LLM.
In this project we get 2 inferences from on-chain LLM and allow the users to rank which of the two responses is better.
The next step would be to fine-tune the model using the dataset generated by this project.

## How it works
The LLM runs as an oprimistic rollup onto EVM chain.
Cartesi VM allows running linux applications, in our case, we run llama2.c with stories15m model.
The Cartesi Rollup infrastructure optimistically executes the transitions between the states of the Cartesi VM.
After finishing running the LLM, Cartesi creates vouchers. 
These vouchers allow to validate and post the result of running the LLM on to the EVM; for that the vouchers need to be executed as an EVM transaction.

## User flow
1. Enter the prompt and run 2 LLM **inferences** by clicking "Generate" button and signing the transaction.
    1. (optionally) enter the total number of tokens that llm will work with = tokens you entered + tokens that LLM will generate
1. When LLM is done, you will see "Off-chain" reponses in the table. You will need to wait for the voucher proofs to be ready. Once the proofs are ready, you can **post** the LLM responses on to the chain by clicking "Post 0" and "Post 1".
1. Now you can **rank** the responses. If you like the order of the responses, you can click "Confirm" if you prefer to switch them, you can click "Switch".
1. When you looped over the previous steps enough times to have a dataset, you can download the dataset as a TSV or JSON. Next you can do the RLHF fine-tuning of the model using other projects.

## Deploy the contract and Cartesi

## Run front-end

after modifying a contract

```
cd trust-and-teach-cartesi
docker compose -f docker-compose.yml -f docker-compose.override.yml down -v ; docker image rm trust-and-teach-cartesi-contracts
docker buildx bake -f docker-bake.hcl -f docker-bake.override.hcl --load && docker compose -f docker-compose.yml -f docker-compose.override.yml up
solc --abi --optimize --base-path . --include-path node_modules/ ../trust-and-teach-cartesi/contracts/src/localhost/trust-and-teach.sol -o src/contract_abi/
cd src/contract_abi/
mv TrustAndTeach.abi TrustAndTeach.json
```



**milestone: splitting a payload into multiple vouchers**
works:
- split LLM response into multiple vouchers
- automated testing for multiple vouchers
(details below)
Problems I'm currently working on:
- even though 200+ random :alpha: + space character strings work, the llama2.c inference doesn't post a notice.
Todo: 
- run on test net
- write docs
- simple ui

